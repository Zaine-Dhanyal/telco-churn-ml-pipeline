{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDAPiFu-XomG",
        "outputId": "15b32972-7a32-47c1-9079-4895fee03cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TELCO CUSTOMER CHURN PREDICTION PIPELINE\n",
            "======================================================================\n",
            "\n",
            "[STEP 1] Loading Telco Churn Dataset...\n",
            "✓ Dataset loaded successfully!\n",
            "  Shape: 7043 rows × 21 columns\n",
            "\n",
            "[Dataset Overview]\n",
            "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
            "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
            "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
            "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
            "3  7795-CFOCW    Male              0      No         No      45           No   \n",
            "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
            "\n",
            "      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n",
            "0  No phone service             DSL             No  ...               No   \n",
            "1                No             DSL            Yes  ...              Yes   \n",
            "2                No             DSL            Yes  ...               No   \n",
            "3  No phone service             DSL            Yes  ...              Yes   \n",
            "4                No     Fiber optic             No  ...               No   \n",
            "\n",
            "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
            "0          No          No              No  Month-to-month              Yes   \n",
            "1          No          No              No        One year               No   \n",
            "2          No          No              No  Month-to-month              Yes   \n",
            "3         Yes          No              No        One year               No   \n",
            "4          No          No              No  Month-to-month              Yes   \n",
            "\n",
            "               PaymentMethod MonthlyCharges  TotalCharges Churn  \n",
            "0           Electronic check          29.85         29.85    No  \n",
            "1               Mailed check          56.95        1889.5    No  \n",
            "2               Mailed check          53.85        108.15   Yes  \n",
            "3  Bank transfer (automatic)          42.30       1840.75    No  \n",
            "4           Electronic check          70.70        151.65   Yes  \n",
            "\n",
            "[5 rows x 21 columns]\n",
            "\n",
            "[Dataset Info]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7043 entries, 0 to 7042\n",
            "Data columns (total 21 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   customerID        7043 non-null   object \n",
            " 1   gender            7043 non-null   object \n",
            " 2   SeniorCitizen     7043 non-null   int64  \n",
            " 3   Partner           7043 non-null   object \n",
            " 4   Dependents        7043 non-null   object \n",
            " 5   tenure            7043 non-null   int64  \n",
            " 6   PhoneService      7043 non-null   object \n",
            " 7   MultipleLines     7043 non-null   object \n",
            " 8   InternetService   7043 non-null   object \n",
            " 9   OnlineSecurity    7043 non-null   object \n",
            " 10  OnlineBackup      7043 non-null   object \n",
            " 11  DeviceProtection  7043 non-null   object \n",
            " 12  TechSupport       7043 non-null   object \n",
            " 13  StreamingTV       7043 non-null   object \n",
            " 14  StreamingMovies   7043 non-null   object \n",
            " 15  Contract          7043 non-null   object \n",
            " 16  PaperlessBilling  7043 non-null   object \n",
            " 17  PaymentMethod     7043 non-null   object \n",
            " 18  MonthlyCharges    7043 non-null   float64\n",
            " 19  TotalCharges      7043 non-null   object \n",
            " 20  Churn             7043 non-null   object \n",
            "dtypes: float64(1), int64(2), object(18)\n",
            "memory usage: 1.1+ MB\n",
            "None\n",
            "\n",
            "======================================================================\n",
            "[STEP 2] Data Preprocessing...\n",
            "======================================================================\n",
            "\n",
            "[2.1] Handling missing values...\n",
            "✓ Missing values handled. Total missing: 0\n",
            "✓ Dropped 'customerID' column\n",
            "\n",
            "[2.2] Encoding target variable (Churn)...\n",
            "✓ Churn encoded: No=0, Yes=1\n",
            "  Class distribution: {0: 5174, 1: 1869}\n",
            "\n",
            "[2.3] Feature types identified:\n",
            "  Numeric features (4): ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']\n",
            "  Categorical features (15): ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
            "\n",
            "======================================================================\n",
            "[STEP 3] Splitting data into train and test sets...\n",
            "======================================================================\n",
            "✓ Data split completed:\n",
            "  Training set: 5634 samples\n",
            "  Test set: 1409 samples\n",
            "  Train churn rate: 26.54%\n",
            "  Test churn rate: 26.54%\n",
            "\n",
            "======================================================================\n",
            "[STEP 4] Creating preprocessing pipeline...\n",
            "======================================================================\n",
            "✓ Preprocessing pipeline created:\n",
            "  - Numeric features: StandardScaler\n",
            "  - Categorical features: OneHotEncoder\n",
            "\n",
            "======================================================================\n",
            "[STEP 5] Creating model pipelines...\n",
            "======================================================================\n",
            "✓ Created two pipelines:\n",
            "  1. Logistic Regression Pipeline\n",
            "  2. Random Forest Pipeline\n",
            "\n",
            "======================================================================\n",
            "[STEP 6] Hyperparameter tuning with GridSearchCV...\n",
            "======================================================================\n",
            "\n",
            "[6.1] Tuning Logistic Regression...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "✓ Logistic Regression tuning complete!\n",
            "  Best parameters: {'classifier__C': 10, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n",
            "  Best CV F1-score: 0.5992\n",
            "\n",
            "[6.2] Tuning Random Forest...\n",
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "✓ Random Forest tuning complete!\n",
            "  Best parameters: {'classifier__max_depth': 10, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 50}\n",
            "  Best CV F1-score: 0.5790\n",
            "\n",
            "======================================================================\n",
            "[STEP 7] Evaluating models on test set...\n",
            "======================================================================\n",
            "\n",
            "[Logistic Regression]\n",
            "--------------------------------------------------\n",
            "Accuracy:  0.8055\n",
            "F1-Score:  0.6040\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    No Churn       0.85      0.89      0.87      1035\n",
            "       Churn       0.66      0.56      0.60       374\n",
            "\n",
            "    accuracy                           0.81      1409\n",
            "   macro avg       0.75      0.73      0.74      1409\n",
            "weighted avg       0.80      0.81      0.80      1409\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[926 109]\n",
            " [165 209]]\n",
            "\n",
            "[Random Forest]\n",
            "--------------------------------------------------\n",
            "Accuracy:  0.8048\n",
            "F1-Score:  0.5877\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    No Churn       0.84      0.91      0.87      1035\n",
            "       Churn       0.67      0.52      0.59       374\n",
            "\n",
            "    accuracy                           0.80      1409\n",
            "   macro avg       0.75      0.72      0.73      1409\n",
            "weighted avg       0.79      0.80      0.80      1409\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[938  97]\n",
            " [178 196]]\n",
            "\n",
            "======================================================================\n",
            "BEST MODEL: Logistic Regression\n",
            "  Accuracy: 0.8055\n",
            "  F1-Score: 0.6040\n",
            "======================================================================\n",
            "\n",
            "[STEP 8] Creating visualizations...\n",
            "✓ Visualization saved: churn_model_evaluation.png\n",
            "\n",
            "======================================================================\n",
            "[STEP 9] Saving the complete pipeline...\n",
            "======================================================================\n",
            "✓ Pipeline saved: /content/telco_churn_pipeline.pkl\n",
            "✓ Label encoder saved: /content/label_encoder.pkl\n",
            "\n",
            "[9.1] Verifying saved pipeline...\n",
            "✓ Pipeline loaded and tested successfully!\n",
            "  Sample predictions: [0 1 0 0 0]\n",
            "\n",
            "======================================================================\n",
            "[STEP 10] Example: Making predictions with the pipeline\n",
            "======================================================================\n",
            "\n",
            "# Load the pipeline\n",
            "import joblib\n",
            "pipeline = joblib.load('telco_churn_pipeline.pkl')\n",
            "\n",
            "# Make predictions on new data\n",
            "predictions = pipeline.predict(new_customer_data)\n",
            "\n",
            "# Get prediction probabilities\n",
            "probabilities = pipeline.predict_proba(new_customer_data)\n",
            "\n",
            "[Live Example]\n",
            "\n",
            "Customer 1:\n",
            "  Prediction: No\n",
            "  Churn Probability: 4.80%\n",
            "\n",
            "Customer 2:\n",
            "  Prediction: Yes\n",
            "  Churn Probability: 68.22%\n",
            "\n",
            "Customer 3:\n",
            "  Prediction: No\n",
            "  Churn Probability: 4.97%\n",
            "\n",
            "======================================================================\n",
            "PIPELINE EXECUTION COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "Summary:\n",
            "✓ Dataset loaded: 7043 customers\n",
            "✓ Features processed: 4 numeric, 15 categorical\n",
            "✓ Models trained: Logistic Regression & Random Forest\n",
            "✓ Best model: Logistic Regression\n",
            "✓ Test Accuracy: 0.8055\n",
            "✓ Test F1-Score: 0.6040\n",
            "\n",
            "Files created:\n",
            "  1. telco_churn_pipeline.pkl (trained model)\n",
            "  2. label_encoder.pkl (target encoder)\n",
            "  3. churn_model_evaluation.png (visualizations)\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Telco Customer Churn Prediction Pipeline\n",
        "A complete end-to-end machine learning pipeline for predicting customer churn\n",
        "using scikit-learn's Pipeline API with hyperparameter tuning.\n",
        "\n",
        "\"\"\"\n",
        "# IMPORTS\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TELCO CUSTOMER CHURN PREDICTION PIPELINE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# STEP 1: LOAD THE DATASET\n",
        "print(\"\\n[STEP 1] Loading Telco Churn Dataset...\")\n",
        "\n",
        "# Download the dataset from a reliable source\n",
        "url = \"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(url)\n",
        "    print(f\" Dataset loaded successfully!\")\n",
        "    print(f\"  Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
        "except Exception as e:\n",
        "    print(f\" Error loading dataset: {e}\")\n",
        "    print(\"  Creating sample dataset for demonstration...\")\n",
        "    # Create a small sample dataset if download fails\n",
        "    np.random.seed(RANDOM_STATE)\n",
        "    n_samples = 1000\n",
        "    df = pd.DataFrame({\n",
        "        'gender': np.random.choice(['Male', 'Female'], n_samples),\n",
        "        'SeniorCitizen': np.random.choice([0, 1], n_samples),\n",
        "        'Partner': np.random.choice(['Yes', 'No'], n_samples),\n",
        "        'Dependents': np.random.choice(['Yes', 'No'], n_samples),\n",
        "        'tenure': np.random.randint(0, 73, n_samples),\n",
        "        'PhoneService': np.random.choice(['Yes', 'No'], n_samples),\n",
        "        'MultipleLines': np.random.choice(['Yes', 'No', 'No phone service'], n_samples),\n",
        "        'InternetService': np.random.choice(['DSL', 'Fiber optic', 'No'], n_samples),\n",
        "        'OnlineSecurity': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
        "        'OnlineBackup': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
        "        'DeviceProtection': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
        "        'TechSupport': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
        "        'StreamingTV': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
        "        'StreamingMovies': np.random.choice(['Yes', 'No', 'No internet service'], n_samples),\n",
        "        'Contract': np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples),\n",
        "        'PaperlessBilling': np.random.choice(['Yes', 'No'], n_samples),\n",
        "        'PaymentMethod': np.random.choice(['Electronic check', 'Mailed check', 'Bank transfer', 'Credit card'], n_samples),\n",
        "        'MonthlyCharges': np.random.uniform(18, 120, n_samples),\n",
        "        'TotalCharges': np.random.uniform(18, 8500, n_samples).astype(str),\n",
        "        'Churn': np.random.choice(['Yes', 'No'], n_samples, p=[0.27, 0.73])\n",
        "    })\n",
        "\n",
        "# Display basic information\n",
        "print(\"\\n[Dataset Overview]\")\n",
        "print(df.head())\n",
        "print(\"\\n[Dataset Info]\")\n",
        "print(df.info())\n",
        "\n",
        "# STEP 2: DATA PREPROCESSING\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[STEP 2] Data Preprocessing...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Handle missing values in TotalCharges (often stored as spaces)\n",
        "print(\"\\n[2.1] Handling missing values...\")\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
        "print(f\" Missing values handled. Total missing: {df.isnull().sum().sum()}\")\n",
        "\n",
        "# Drop customerID if it exists (not useful for prediction)\n",
        "if 'customerID' in df.columns:\n",
        "    df.drop('customerID', axis=1, inplace=True)\n",
        "    print(\" Dropped 'customerID' column\")\n",
        "\n",
        "# Encode target variable\n",
        "print(\"\\n[2.2] Encoding target variable (Churn)...\")\n",
        "label_encoder = LabelEncoder()\n",
        "df['Churn'] = label_encoder.fit_transform(df['Churn'])\n",
        "print(f\" Churn encoded: No=0, Yes=1\")\n",
        "print(f\"  Class distribution: {df['Churn'].value_counts().to_dict()}\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"\\n[2.3] Feature types identified:\")\n",
        "print(f\"  Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
        "print(f\"  Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
        "\n",
        "# STEP 3: SPLIT DATA\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[STEP 3] Splitting data into train and test sets...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"✓ Data split completed:\")\n",
        "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"  Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"  Train churn rate: {y_train.mean():.2%}\")\n",
        "print(f\"  Test churn rate: {y_test.mean():.2%}\")\n",
        "\n",
        "# STEP 4: CREATE PREPROCESSING PIPELINE\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[STEP 4] Creating preprocessing pipeline...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Numeric transformer: scale numeric features\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical transformer: one-hot encode categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "# Combine transformers into a ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\" Preprocessing pipeline created:\")\n",
        "print(f\"  - Numeric features: StandardScaler\")\n",
        "print(f\"  - Categorical features: OneHotEncoder\")\n",
        "\n",
        "# STEP 5: CREATE MODEL PIPELINES\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[STEP 5] Creating model pipelines...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Pipeline 1: Logistic Regression\n",
        "lr_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(random_state=RANDOM_STATE, max_iter=1000))\n",
        "])\n",
        "\n",
        "# Pipeline 2: Random Forest\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "print(\" Created two pipelines:\")\n",
        "print(\"  1. Logistic Regression Pipeline\")\n",
        "print(\"  2. Random Forest Pipeline\")\n",
        "\n",
        "# STEP 6: HYPERPARAMETER TUNING WITH GRIDSEARCHCV\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[STEP 6] Hyperparameter tuning with GridSearchCV...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Define parameter grids for each model\n",
        "print(\"\\n[6.1] Tuning Logistic Regression...\")\n",
        "\n",
        "lr_param_grid = {\n",
        "    'classifier__C': [0.01, 0.1, 1, 10],\n",
        "    'classifier__penalty': ['l2'],\n",
        "    'classifier__solver': ['lbfgs', 'liblinear']\n",
        "}\n",
        "\n",
        "lr_grid_search = GridSearchCV(\n",
        "    lr_pipeline,\n",
        "    param_grid=lr_param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "lr_grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\" Logistic Regression tuning complete!\")\n",
        "print(f\"  Best parameters: {lr_grid_search.best_params_}\")\n",
        "print(f\"  Best CV F1-score: {lr_grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Tuning Random Forest\n",
        "print(\"\\n[6.2] Tuning Random Forest...\")\n",
        "\n",
        "rf_param_grid = {\n",
        "    'classifier__n_estimators': [50, 100, 200],\n",
        "    'classifier__max_depth': [10, 20, None],\n",
        "    'classifier__min_samples_split': [2, 5],\n",
        "    'classifier__min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "rf_grid_search = GridSearchCV(\n",
        "    rf_pipeline,\n",
        "    param_grid=rf_param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\" Random Forest tuning complete!\")\n",
        "print(f\"  Best parameters: {rf_grid_search.best_params_}\")\n",
        "print(f\"  Best CV F1-score: {rf_grid_search.best_score_:.4f}\")\n",
        "\n",
        "# STEP 7: MODEL EVALUATION\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[STEP 7] Evaluating models on test set...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Evaluate both models\n",
        "models = {\n",
        "    'Logistic Regression': lr_grid_search,\n",
        "    'Random Forest': rf_grid_search\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n[{name}]\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    results[name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'f1_score': f1,\n",
        "        'model': model\n",
        "    }\n",
        "\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "# Select best model based on F1-score\n",
        "best_model_name = max(results, key=lambda x: results[x]['f1_score'])\n",
        "best_model = results[best_model_name]['model']\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"BEST MODEL: {best_model_name}\")\n",
        "print(f\"  Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
        "print(f\"  F1-Score: {results[best_model_name]['f1_score']:.4f}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# STEP 8: VISUALIZATIONS\n",
        "print(\"\\n[STEP 8] Creating visualizations...\")\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Model Comparison\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'Accuracy': [results[m]['accuracy'] for m in results.keys()],\n",
        "    'F1-Score': [results[m]['f1_score'] for m in results.keys()]\n",
        "})\n",
        "\n",
        "x_pos = np.arange(len(metrics_df))\n",
        "width = 0.35\n",
        "\n",
        "axes[0].bar(x_pos - width/2, metrics_df['Accuracy'], width, label='Accuracy', alpha=0.8)\n",
        "axes[0].bar(x_pos + width/2, metrics_df['F1-Score'], width, label='F1-Score', alpha=0.8)\n",
        "axes[0].set_xlabel('Model', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xticks(x_pos)\n",
        "axes[0].set_xticklabels(metrics_df['Model'], rotation=15, ha='right')\n",
        "axes[0].legend()\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "axes[0].set_ylim([0, 1])\n",
        "\n",
        "# Plot 2: Confusion Matrix for Best Model\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "cm_best = confusion_matrix(y_test, y_pred_best)\n",
        "\n",
        "sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
        "            xticklabels=['No Churn', 'Churn'],\n",
        "            yticklabels=['No Churn', 'Churn'])\n",
        "axes[1].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/churn_model_evaluation.png', dpi=300, bbox_inches='tight')\n",
        "print(\" Visualization saved: churn_model_evaluation.png\")\n",
        "plt.close()\n",
        "\n",
        "# STEP 9: SAVE THE PIPELINE\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[STEP 9] Saving the complete pipeline...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Save the best model pipeline\n",
        "pipeline_filename = '/content/telco_churn_pipeline.pkl'\n",
        "joblib.dump(best_model, pipeline_filename)\n",
        "print(f\" Pipeline saved: {pipeline_filename}\")\n",
        "\n",
        "# Save label encoder as well\n",
        "encoder_filename = '/content/label_encoder.pkl'\n",
        "joblib.dump(label_encoder, encoder_filename)\n",
        "print(f\" Label encoder saved: {encoder_filename}\")\n",
        "\n",
        "# Verify the saved pipeline works\n",
        "print(\"\\n[9.1] Verifying saved pipeline...\")\n",
        "loaded_pipeline = joblib.load(pipeline_filename)\n",
        "test_predictions = loaded_pipeline.predict(X_test[:5])\n",
        "print(f\" Pipeline loaded and tested successfully!\")\n",
        "print(f\"  Sample predictions: {test_predictions}\")\n",
        "\n",
        "# STEP 10: USAGE EXAMPLE\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"[STEP 10] Example: Making predictions with the pipeline\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Example of how to use the saved pipeline\n",
        "print(\"\\n# Load the pipeline\")\n",
        "print(\"import joblib\")\n",
        "print(\"pipeline = joblib.load('telco_churn_pipeline.pkl')\")\n",
        "print(\"\\n# Make predictions on new data\")\n",
        "print(\"predictions = pipeline.predict(new_customer_data)\")\n",
        "print(\"\\n# Get prediction probabilities\")\n",
        "print(\"probabilities = pipeline.predict_proba(new_customer_data)\")\n",
        "\n",
        "# Demonstrate with actual data\n",
        "print(\"\\n[Live Example]\")\n",
        "sample_customer = X_test.iloc[:3]\n",
        "predictions = loaded_pipeline.predict(sample_customer)\n",
        "probabilities = loaded_pipeline.predict_proba(sample_customer)\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "    churn_label = label_encoder.inverse_transform([predictions[i]])[0]\n",
        "    churn_prob = probabilities[i][1]\n",
        "    print(f\"\\nCustomer {i+1}:\")\n",
        "    print(f\"  Prediction: {churn_label}\")\n",
        "    print(f\"  Churn Probability: {churn_prob:.2%}\")\n",
        "\n",
        "# FINAL SUMMARY\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PIPELINE EXECUTION COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nSummary:\")\n",
        "print(f\" Dataset loaded: {df.shape[0]} customers\")\n",
        "print(f\" Features processed: {len(numeric_features)} numeric, {len(categorical_features)} categorical\")\n",
        "print(f\" Models trained: Logistic Regression & Random Forest\")\n",
        "print(f\" Best model: {best_model_name}\")\n",
        "print(f\" Test Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
        "print(f\" Test F1-Score: {results[best_model_name]['f1_score']:.4f}\")\n",
        "print(f\"\\nFiles created:\")\n",
        "print(f\"  1. telco_churn_pipeline.pkl (trained model)\")\n",
        "print(f\"  2. label_encoder.pkl (target encoder)\")\n",
        "print(f\"  3. churn_model_evaluation.png (visualizations)\")\n",
        "print(\"\\n\" + \"=\" * 70)"
      ]
    }
  ]
}